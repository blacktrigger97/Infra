networks:
  bdCluster:
    name: bdCluster
    driver: ipvlan
    external: true
    driver_opts:
      parent: ${ETHERNET_NAME}
    ipam:
      config:
        - subnet: 192.168.1.0/24
          ip_range: 192.168.1.0/24
          gateway: 192.168.1.1

volumes:
  cifs-volume-airflow:
    driver_opts:
      type: cifs
      o: nounix,rw
      device: //worker1.bdc.home/airflow

services:

  x-airflow-common: &airflow-common
    build:
      dockerfile: Dockerfile
      args:
        - AIRFLOW_VER=${AIRFLOW_VER}
        - SPARK_VERSION=${SPARK_VERSION}
        - DOCKER_DIR=${DOCKER_DIR}
        - GIT_CONFIG_DIR=${GIT_CONFIG_DIR}
        - GIT_CONFIG_ADDRESS=${GIT_CONFIG_ADDRESS}
    restart: unless-stopped
    ports:
      - 8080:8080
      - 5555:5555
      - 8794:8794
      - 8793:8793
    environment:
      TZ: "Asia/Kolkata"
    volumes:
      - cifs-volume-airflow:/opt/airflow:z

  api-server:
    <<: *airflow-common
    container_name: airflow-server
    command: bash -c "airflow api-server -H `hostname` && airflow celery flower --hostname `hostname`"
    hostname: airflow-server.bdc.home
    restart: unless-stopped
    networks:
      bdCluster:
        ipv4_address: 192.168.1.61
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://airflow-server.bdc.home:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - scheduler

  scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: bash -c "airflow db migrate && airflow users create --username airflow --firstname Mayank --lastname Singh --role Admin --email dummy --password airflow && airflow scheduler"
    hostname: airflow-scheduler.bdc.home
    restart: unless-stopped
    networks:
      bdCluster:
        ipv4_address: 192.168.1.62
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://airflow-scheduler.bdc.home:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  dag-processor:
    <<: *airflow-common
    container_name: airflow-dagprocessor
    command: bash -c "airflow dag-processor && airflow triggerer --hostname `hostname`"
    hostname: airflow-dagprocessor.bdc.home
    restart: unless-stopped
    networks:
      bdCluster:
        ipv4_address: 192.168.1.63
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - api-server

  airflow-wk1:
    <<: *airflow-common
    container_name: airflow-wk1
    hostname: airflow-wk1.bdc.home
    command: bash -c "airflow celery worker -H `hostname`"
    networks:
      bdCluster:
        ipv4_address: 192.168.1.121
    # ports:
    #   - 8793:8793
    # volumes:
    #   - cifs-volume-airflow:/root/airflow:z

  airflow-wk2:
    <<: *airflow-common
    container_name: airflow-wk2
    hostname: airflow-wk2.bdc.home
    command: bash -c "airflow celery worker -H `hostname`"
    networks:
      bdCluster:
        ipv4_address: 192.168.1.122
    # ports:
    #   - 8793:8793
    # volumes:
    #   - cifs-volume-airflow:/root/airflow:z

  airflow-wk3:
    <<: *airflow-common
    container_name: airflow-wk3
    hostname: airflow-wk3.bdc.home
    command: bash -c "airflow celery worker -H `hostname`"
    networks:
      bdCluster:
        ipv4_address: 192.168.1.123
    # ports:
    #   - 8793:8793
    # volumes:
    #   - cifs-volume-airflow:/root/airflow:z
